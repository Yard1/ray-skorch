{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poker rule induction with TrainSklearn and FastAI TabularModel\n",
    "\n",
    "Requires `fastai`. Data from https://www.kaggle.com/c/poker-rule-induction/data?select=train.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "import ray\n",
    "\n",
    "from torch import nn\n",
    "from skorch.callbacks import LRScheduler, GradientNormClipping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from fastai.tabular.model import TabularModel, emb_sz_rule\n",
    "\n",
    "from train_sklearn import RayTrainNeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine some fastai embedding functions to work with pandas\n",
    "\n",
    "def _one_emb_sz(n_cat, n):\n",
    "    \"Pick an embedding size for `n` depending on `classes` if not given in `sz_dict`.\"\n",
    "    sz_dict = {}\n",
    "    sz = sz_dict.get(n, int(emb_sz_rule(n_cat)))  # rule of thumb\n",
    "    return n_cat, sz\n",
    "\n",
    "\n",
    "def get_emb_sz(sizes: list, columns: list):\n",
    "    \"Get default embedding size from `TabularPreprocessor` `proc` or the ones in `sz_dict`\"\n",
    "\n",
    "    return tuple(_one_emb_sz(size, column) for size, column in zip(sizes, columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data from https://www.kaggle.com/c/poker-rule-induction/data?select=train.csv.zip\n",
    "\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "target = data[\"hand\"]\n",
    "\n",
    "# Index categories from 0\n",
    "data = data.drop(\"hand\", axis=1)-1\n",
    "\n",
    "cat_cols = data.shape[1]\n",
    "cat_cols_names = data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add numerical versions of C* columns - helps the network learn the order\n",
    "\n",
    "num_cols = pd.concat([data[col] for col in data.columns if col.startswith(\"C\")], axis=1)\n",
    "num_cols.columns = [f\"n{col}\" for col in num_cols.columns]\n",
    "num_cols_names = num_cols.columns\n",
    "data = pd.concat((data, num_cols),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, train_label, val_label = train_test_split(data, target, test_size=0.2, random_state=1)\n",
    "scaler = StandardScaler()\n",
    "train_data[num_cols.columns] = scaler.fit_transform(train_data[num_cols.columns]).astype(\"double\")\n",
    "val_data[num_cols.columns] = scaler.transform(val_data[num_cols.columns]).astype(\"double\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_szs = get_emb_sz([len(data[col].unique()) for col in cat_cols_names], list(cat_cols_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.31.43.110',\n",
       " 'raylet_ip_address': '172.31.43.110',\n",
       " 'redis_address': '172.31.43.110:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2021-12-15_21-12-28_829185_80136/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-12-15_21-12-28_829185_80136/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2021-12-15_21-12-28_829185_80136',\n",
       " 'metrics_export_port': 62611,\n",
       " 'node_id': '41670c44d3cbacf457c521bfb5742f8154f2cec91c0e11f7a5209138'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(ignore_reinit_error=True)  # specify address= if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 2\n",
    "batch_size = 512 // 2\n",
    "epochs = 200\n",
    "device = \"cpu\"\n",
    "lr = 0.01\n",
    "\n",
    "reg = RayTrainNeuralNet(\n",
    "    TabularModel,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    callbacks=[\n",
    "        GradientNormClipping(1.0),\n",
    "        LRScheduler(\n",
    "            torch.optim.lr_scheduler.OneCycleLR,\n",
    "            max_lr=lr*10,\n",
    "            step_every=\"batch\",\n",
    "            pct_start=0.25,\n",
    "            final_div_factor=100000.0,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=math.ceil(train_data.shape[0] / batch_size))\n",
    "    ],\n",
    "    num_workers=num_workers,\n",
    "    max_epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    device=device,\n",
    "    optimizer__weight_decay=1e-5,\n",
    "    # network configuration\n",
    "    module__emb_szs=emb_szs,\n",
    "    module__n_cont=len(num_cols_names),\n",
    "    module__out_sz=10,\n",
    "    module__layers=[100, 50, 50],\n",
    "    module__ps=[0.01, 0.01, 0.02],\n",
    "    # squeezing required for CrossEntropyLoss\n",
    "    iterator_train__unsqueeze_label_tensor=False,\n",
    "    iterator_valid__unsqueeze_label_tensor=False,\n",
    "    # set correct dtypes\n",
    "    iterator_train__feature_column_dtypes={\n",
    "        \"x_cat\": [torch.long] * len(cat_cols_names),\n",
    "        \"x_cont\": [torch.float] * len(num_cols_names)\n",
    "    },\n",
    "    iterator_valid__feature_column_dtypes={\n",
    "        \"x_cat\": [torch.long] * len(cat_cols_names),\n",
    "        \"x_cont\": [torch.float] * len(num_cols_names)\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-15 21:14:33,594\tINFO trainer.py:172 -- Trainer logs will be logged in: /home/ubuntu/ray_results/train_2021-12-15_21-14-33\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=136803)\u001b[0m 2021-12-15 21:14:35,208\tINFO torch.py:66 -- Setting up process group for: env:// [rank=1, world_size=2]\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=136804)\u001b[0m 2021-12-15 21:14:35,203\tINFO torch.py:66 -- Setting up process group for: env:// [rank=0, world_size=2]\n",
      "2021-12-15 21:14:35,229\tINFO trainer.py:178 -- Run results will be logged in: /home/ubuntu/ray_results/train_2021-12-15_21-14-33/run_001\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=136803)\u001b[0m 2021-12-15 21:14:38,443\tINFO torch.py:239 -- Moving model to device: cpu\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=136803)\u001b[0m 2021-12-15 21:14:38,443\tINFO torch.py:242 -- Wrapping provided model in DDP.\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=136804)\u001b[0m 2021-12-15 21:14:38,434\tINFO torch.py:239 -- Moving model to device: cpu\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=136804)\u001b[0m 2021-12-15 21:14:38,435\tINFO torch.py:242 -- Wrapping provided model in DDP.\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=136803)\u001b[0m [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=136804)\u001b[0m [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    dur_s\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m1.8393\u001b[0m        \u001b[32m1.1965\u001b[0m   2.0558\n",
      "      2        \u001b[36m1.0144\u001b[0m        \u001b[32m0.9368\u001b[0m   1.1105\n",
      "      3        \u001b[36m0.9235\u001b[0m        \u001b[32m0.9162\u001b[0m   0.6768\n",
      "      4        \u001b[36m0.9021\u001b[0m        \u001b[32m0.9087\u001b[0m   0.7174\n",
      "      5        \u001b[36m0.8879\u001b[0m        \u001b[32m0.9012\u001b[0m   0.7954\n",
      "      6        \u001b[36m0.8690\u001b[0m        \u001b[32m0.8795\u001b[0m   0.8108\n",
      "      7        \u001b[36m0.8348\u001b[0m        \u001b[32m0.8621\u001b[0m   0.9922\n",
      "      8        \u001b[36m0.8058\u001b[0m        \u001b[32m0.8378\u001b[0m   0.9411\n",
      "      9        \u001b[36m0.7748\u001b[0m        \u001b[32m0.8285\u001b[0m   0.8090\n",
      "     10        \u001b[36m0.7373\u001b[0m        \u001b[32m0.7751\u001b[0m   0.9506\n",
      "     11        \u001b[36m0.6791\u001b[0m        \u001b[32m0.7250\u001b[0m   0.8650\n",
      "     12        \u001b[36m0.6295\u001b[0m        \u001b[32m0.7011\u001b[0m   0.7355\n",
      "     13        \u001b[36m0.5888\u001b[0m        \u001b[32m0.6692\u001b[0m   0.7873\n",
      "     14        \u001b[36m0.5425\u001b[0m        \u001b[32m0.6519\u001b[0m   0.7971\n",
      "     15        \u001b[36m0.5076\u001b[0m        \u001b[32m0.5976\u001b[0m   0.7339\n",
      "     16        \u001b[36m0.4590\u001b[0m        \u001b[32m0.5672\u001b[0m   0.8308\n",
      "     17        \u001b[36m0.4295\u001b[0m        \u001b[32m0.5323\u001b[0m   0.9556\n",
      "     18        \u001b[36m0.3999\u001b[0m        \u001b[32m0.4808\u001b[0m   0.8554\n",
      "     19        \u001b[36m0.3679\u001b[0m        0.4819   0.7445\n",
      "     20        \u001b[36m0.3560\u001b[0m        0.4923   0.8963\n",
      "     21        \u001b[36m0.3410\u001b[0m        0.4725   0.7973\n",
      "     22        \u001b[36m0.3337\u001b[0m        \u001b[32m0.4636\u001b[0m   0.6998\n",
      "     23        \u001b[36m0.3195\u001b[0m        \u001b[32m0.4408\u001b[0m   0.7497\n",
      "     24        \u001b[36m0.3185\u001b[0m        \u001b[32m0.4321\u001b[0m   0.8048\n",
      "     25        \u001b[36m0.2967\u001b[0m        \u001b[32m0.4361\u001b[0m   0.7886\n",
      "     26        \u001b[36m0.2878\u001b[0m        \u001b[32m0.4278\u001b[0m   0.8176\n",
      "     27        \u001b[36m0.2735\u001b[0m        \u001b[32m0.4247\u001b[0m   1.3318\n",
      "     28        \u001b[36m0.2755\u001b[0m        0.4194   1.5234\n",
      "     29        \u001b[36m0.2638\u001b[0m        \u001b[32m0.4006\u001b[0m   1.2552\n",
      "     30        0.2698        \u001b[32m0.3936\u001b[0m   1.2643\n",
      "     31        \u001b[36m0.2580\u001b[0m        0.4130   1.2745\n",
      "     32        \u001b[36m0.2476\u001b[0m        0.4163   1.0441\n",
      "     33        0.2542        \u001b[32m0.3830\u001b[0m   1.2222\n",
      "     34        0.2472        \u001b[32m0.3771\u001b[0m   0.8958\n",
      "     35        \u001b[36m0.2351\u001b[0m        0.4329   0.8647\n",
      "     36        \u001b[36m0.2274\u001b[0m        0.3928   0.6549\n",
      "     37        0.2308        \u001b[32m0.3566\u001b[0m   0.6984\n",
      "     38        \u001b[36m0.2131\u001b[0m        0.3979   0.7119\n",
      "     39        \u001b[36m0.2059\u001b[0m        0.3784   0.8258\n",
      "     40        \u001b[36m0.1896\u001b[0m        0.3548   0.8941\n",
      "     41        0.1908        0.4231   0.7688\n",
      "     42        0.1959        0.3988   0.6882\n",
      "     43        0.1875        0.3673   0.8790\n",
      "     44        0.1996        0.3503   0.8558\n",
      "     45        \u001b[36m0.1750\u001b[0m        \u001b[32m0.3338\u001b[0m   0.8083\n",
      "     46        \u001b[36m0.1733\u001b[0m        0.3522   0.6332\n",
      "     47        0.1895        0.3775   0.6766\n",
      "     48        \u001b[36m0.1544\u001b[0m        \u001b[32m0.2796\u001b[0m   0.6937\n",
      "     49        0.1734        0.3438   0.8303\n",
      "     50        \u001b[36m0.1602\u001b[0m        0.2932   0.8845\n",
      "     51        \u001b[36m0.1326\u001b[0m        \u001b[32m0.2978\u001b[0m   0.6948\n",
      "     52        0.1682        \u001b[32m0.2510\u001b[0m   0.6987\n",
      "     53        \u001b[36m0.1344\u001b[0m        \u001b[32m0.2313\u001b[0m   1.0669\n",
      "     54        \u001b[36m0.1244\u001b[0m        0.2697   0.8842\n",
      "     55        \u001b[36m0.1198\u001b[0m        0.2509   0.8411\n",
      "     56        0.1419        0.2914   0.9454\n",
      "     57        0.1648        0.2725   0.8051\n",
      "     58        0.1684        0.2775   0.8152\n",
      "     59        0.1514        0.2541   0.7538\n",
      "     60        \u001b[36m0.1147\u001b[0m        0.2813   0.7605\n",
      "     61        0.1407        0.2799   0.7823\n",
      "     62        0.1687        0.2760   0.7643\n",
      "     63        0.1482        \u001b[32m0.1835\u001b[0m   0.9185\n",
      "     64        \u001b[36m0.1072\u001b[0m        0.1874   0.7925\n",
      "     65        \u001b[36m0.0955\u001b[0m        0.1987   0.7216\n",
      "     66        0.1126        0.2190   0.8283\n",
      "     67        0.1267        \u001b[32m0.1608\u001b[0m   0.8250\n",
      "     68        0.1162        0.1852   0.7587\n",
      "     69        0.1066        0.1862   0.7695\n",
      "     70        0.1239        0.1802   0.7913\n",
      "     71        0.0985        0.1856   0.9323\n",
      "     72        0.0951        0.2153   0.7599\n",
      "     73        0.1053        0.1882   0.7878\n",
      "     74        0.1042        0.1958   0.7052\n",
      "     75        0.1888        \u001b[32m0.1626\u001b[0m   0.7389\n",
      "     76        0.0963        0.1726   0.7703\n",
      "     77        0.0909        \u001b[32m0.1508\u001b[0m   0.8885\n",
      "     78        \u001b[36m0.0834\u001b[0m        0.1584   0.8168\n",
      "     79        \u001b[36m0.0775\u001b[0m        0.1609   0.7911\n",
      "     80        \u001b[36m0.0688\u001b[0m        0.2086   0.7138\n",
      "     81        0.0984        0.1699   0.7561\n",
      "     82        0.0760        0.2539   0.8933\n",
      "     83        0.1195        0.1407   0.7631\n",
      "     84        0.0862        0.1504   0.6934\n",
      "     85        \u001b[36m0.0625\u001b[0m        \u001b[32m0.1226\u001b[0m   0.8469\n",
      "     86        0.0839        0.1433   0.9749\n",
      "     87        0.0763        0.1573   0.8507\n",
      "     88        0.0907        0.1715   0.8267\n",
      "     89        0.1046        0.1676   1.0056\n",
      "     90        0.1025        0.1910   0.8350\n",
      "     91        0.1578        0.1602   0.9671\n",
      "     92        0.0905        0.1862   0.6859\n",
      "     93        0.0852        0.1785   0.7642\n",
      "     94        0.0937        0.2036   0.7872\n",
      "     95        0.2077        0.3031   0.8148\n",
      "     96        0.1570        \u001b[32m0.1294\u001b[0m   0.7369\n",
      "     97        0.0773        0.1408   1.2829\n",
      "     98        0.0679        0.1316   1.2734\n",
      "     99        0.0661        0.1345   1.0893\n",
      "    100        0.0636        0.1482   1.1161\n",
      "    101        0.0654        0.1428   1.0475\n",
      "    102        \u001b[36m0.0600\u001b[0m        0.1431   0.9250\n",
      "    103        \u001b[36m0.0664\u001b[0m        0.2044   1.3074\n",
      "    104        0.0894        \u001b[32m0.1304\u001b[0m   0.9872\n",
      "    105        0.0749        0.1365   0.9155\n",
      "    106        \u001b[36m0.0609\u001b[0m        0.1552   0.8528\n",
      "    107        0.0814        \u001b[32m0.1251\u001b[0m   0.8732\n",
      "    108        \u001b[36m0.0439\u001b[0m        \u001b[32m0.1096\u001b[0m   0.8853\n",
      "    109        0.0585        0.2166   1.3732\n",
      "    110        0.0631        0.1414   0.7786\n",
      "    111        0.0563        0.1780   0.8868\n",
      "    112        0.1043        0.1855   0.8053\n",
      "    113        0.0988        0.1315   0.7742\n",
      "    114        0.0578        0.1556   0.9264\n",
      "    115        0.0493        0.1361   0.9076\n",
      "    116        0.0524        0.1546   0.7568\n",
      "    117        0.0608        0.1528   0.7803\n",
      "    118        0.0550        0.1360   0.9011\n",
      "    119        0.0471        0.1550   1.0625\n",
      "    120        0.0460        0.1715   0.8964\n",
      "    121        0.0542        0.1339   1.0033\n",
      "    122        \u001b[36m0.0455\u001b[0m        0.2236   0.9577\n",
      "    123        0.0761        0.1500   0.9110\n",
      "    124        0.1286        0.1469   0.9201\n",
      "    125        0.0812        0.1563   0.9515\n",
      "    126        0.1104        0.2456   0.8560\n",
      "    127        0.0617        0.1526   0.7624\n",
      "    128        0.0611        0.1439   0.8670\n",
      "    129        0.0532        0.1778   0.9187\n",
      "    130        0.0620        0.1362   0.8363\n",
      "    131        0.0475        0.1455   0.7906\n",
      "    132        0.0559        0.1553   0.8621\n",
      "    133        0.0542        0.1611   0.7553\n",
      "    134        0.0462        0.1771   0.7006\n",
      "    135        0.0456        0.1322   0.7013\n",
      "    136        0.0470        0.1350   0.7472\n",
      "    137        \u001b[36m0.0377\u001b[0m        0.1632   0.8733\n",
      "    138        0.0451        0.1935   0.8318\n",
      "    139        0.0968        0.1941   0.7806\n",
      "    140        0.0573        0.1716   0.9047\n",
      "    141        0.0586        0.1956   0.9103\n",
      "    142        0.0606        0.1959   0.7197\n",
      "    143        0.0702        0.1866   0.6974\n",
      "    144        0.0545        0.1245   0.6721\n",
      "    145        \u001b[36m0.0360\u001b[0m        0.1559   1.0239\n",
      "    146        0.0376        0.1897   0.8693\n",
      "    147        \u001b[36m0.0336\u001b[0m        0.1270   0.9125\n",
      "    148        0.0404        0.1514   1.0088\n",
      "    149        0.0506        0.1702   0.9197\n",
      "    150        0.0391        0.1824   0.8941\n",
      "    151        0.0442        0.1832   0.7677\n",
      "    152        0.0471        0.1675   0.7381\n",
      "    153        0.0370        0.1551   0.8819\n",
      "    154        0.0358        0.1741   0.7812\n",
      "    155        0.0338        0.1816   0.7840\n",
      "    156        0.0477        0.1593   0.7631\n",
      "    157        0.0429        0.1624   0.7443\n",
      "    158        0.0465        0.2047   1.0284\n",
      "    159        0.0734        0.1544   0.7917\n",
      "    160        0.0407        0.1719   0.7842\n",
      "    161        0.0524        0.1447   0.8259\n",
      "    162        0.0456        0.1973   0.8681\n",
      "    163        0.0389        0.1332   0.8508\n",
      "    164        0.0545        0.1651   1.0920\n",
      "    165        0.0482        0.1528   1.4418\n",
      "    166        0.0950        \u001b[32m0.1095\u001b[0m   1.3459\n",
      "    167        0.0409        0.1650   1.1937\n",
      "    168        0.0642        0.1715   1.3383\n",
      "    169        0.0842        0.1350   1.3416\n",
      "    170        0.0612        0.1443   1.2872\n",
      "    171        0.0719        0.1802   1.3597\n",
      "    172        0.0865        0.1605   1.4304\n",
      "    173        0.0551        0.2047   0.9354\n",
      "    174        0.0473        0.1445   1.1676\n",
      "    175        0.0418        0.1637   0.8126\n",
      "    176        0.0321        0.1253   0.8700\n",
      "    177        0.0462        0.2459   0.8738\n",
      "    178        0.0433        0.1430   0.9878\n",
      "    179        0.0395        0.1681   1.1245\n",
      "    180        0.0451        0.1601   0.9729\n",
      "    181        0.0341        0.1691   1.1316\n",
      "    182        0.0562        0.2411   1.1750\n",
      "    183        0.0480        0.1735   0.9422\n",
      "    184        0.0335        0.2115   1.0267\n",
      "    185        0.0360        0.1654   0.7887\n",
      "    186        \u001b[36m0.0319\u001b[0m        0.1510   0.7773\n",
      "    187        0.0763        0.1541   0.9353\n",
      "    188        0.0418        0.1622   1.0124\n",
      "    189        0.0343        0.1366   0.9189\n",
      "    190        0.0806        0.1307   1.0344\n",
      "    191        0.0461        0.2154   0.7578\n",
      "    192        \u001b[36m0.0332\u001b[0m        0.1695   0.9545\n",
      "    193        0.0534        0.1520   0.8882\n",
      "    194        0.0497        0.2065   0.8435\n",
      "    195        0.0779        0.1489   0.9038\n",
      "    196        0.0393        0.1246   0.9216\n",
      "    197        0.0335        0.1442   0.8481\n",
      "    198        \u001b[36m0.0333\u001b[0m        0.1490   0.7781\n",
      "    199        0.0554        0.1760   0.8134\n",
      "    200        0.0725        0.1298   0.7275\n",
      "Re-initializing optimizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'train_sklearn.base.RayTrainNeuralNet'>[initialized](\n",
       "  module_=TabularModel(\n",
       "    (embeds): ModuleList(\n",
       "      (0): Embedding(4, 3)\n",
       "      (1): Embedding(13, 7)\n",
       "      (2): Embedding(4, 3)\n",
       "      (3): Embedding(13, 7)\n",
       "      (4): Embedding(4, 3)\n",
       "      (5): Embedding(13, 7)\n",
       "      (6): Embedding(4, 3)\n",
       "      (7): Embedding(13, 7)\n",
       "      (8): Embedding(4, 3)\n",
       "      (9): Embedding(13, 7)\n",
       "    )\n",
       "    (emb_drop): Dropout(p=0.0, inplace=False)\n",
       "    (bn_cont): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (layers): Sequential(\n",
       "      (0): LinBnDrop(\n",
       "        (0): Linear(in_features=55, out_features=100, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Dropout(p=0.01, inplace=False)\n",
       "      )\n",
       "      (1): LinBnDrop(\n",
       "        (0): Linear(in_features=100, out_features=50, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Dropout(p=0.01, inplace=False)\n",
       "      )\n",
       "      (2): LinBnDrop(\n",
       "        (0): Linear(in_features=50, out_features=50, bias=False)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Dropout(p=0.02, inplace=False)\n",
       "      )\n",
       "      (3): LinBnDrop(\n",
       "        (0): Linear(in_features=50, out_features=10, bias=True)\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fastai TabularModel takes separate categorical and numerical feature tensors in forward\n",
    "reg.fit(\n",
    "    {\n",
    "        \"x_cat\": train_data[cat_cols_names],\n",
    "        \"x_cont\": train_data[num_cols_names]\n",
    "    },\n",
    "    train_label,\n",
    "    X_val={\n",
    "        \"x_cat\": val_data[cat_cols_names],\n",
    "        \"x_cont\": val_data[num_cols_names]\n",
    "    },\n",
    "    y_val=val_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=137469)\u001b[0m 2021-12-15 21:17:39,975\tINFO torch.py:66 -- Setting up process group for: env:// [rank=0, world_size=2]\n",
      "2021-12-15 21:17:40,077\tINFO trainer.py:178 -- Run results will be logged in: /home/ubuntu/ray_results/train_2021-12-15_21-14-33/run_002\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=137558)\u001b[0m 2021-12-15 21:17:40,050\tINFO torch.py:66 -- Setting up process group for: env:// [rank=1, world_size=2]\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=137469)\u001b[0m 2021-12-15 21:17:44,288\tINFO torch.py:239 -- Moving model to device: cpu\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=137469)\u001b[0m 2021-12-15 21:17:44,289\tINFO torch.py:242 -- Wrapping provided model in DDP.\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=137558)\u001b[0m 2021-12-15 21:17:44,290\tINFO torch.py:239 -- Moving model to device: cpu\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=137558)\u001b[0m 2021-12-15 21:17:44,290\tINFO torch.py:242 -- Wrapping provided model in DDP.\n"
     ]
    }
   ],
   "source": [
    "X_pred = reg.predict_proba({\n",
    "    \"x_cat\": val_data[cat_cols_names],\n",
    "    \"x_cont\": val_data[num_cols_names]\n",
    "}).to_pandas().idxmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9802079168332667\n"
     ]
    }
   ],
   "source": [
    "# >0.95 accuracy is expected\n",
    "\n",
    "print(f\"Final accuracy: {accuracy_score(val_label, X_pred)}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42cdb777d98d67f3b2fb4b742cd32c61d6d0ef1a589fa1e04937806e156b3db0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
